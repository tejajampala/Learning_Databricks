{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3890f38-5b62-430a-bb6d-4ee097587741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Work Types"
    }
   },
   "outputs": [],
   "source": [
    "# DLT works with 3 types of datasets\n",
    "# Streaming Tables (Permanent/Temporary) - used as Append Data Sources, Incremental data\n",
    "# Materialized views - used for transformation, aggregations or computations\n",
    "# Views - used for intermediate transformations, not stored in target schema\n",
    "\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ea5d5a-e17f-48a5-af98-c4d849abd364",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Streaming table"
    }
   },
   "outputs": [],
   "source": [
    "# create a streaming table for orders\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"bronze\"},\n",
    "  comment = \"Streaming table for orders bronze table\"\n",
    ")\n",
    "def orders_bronze():\n",
    "  df = spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "  return df\n",
    "\n",
    "#@dlt.create_streaming_table(comment = \"Streaming table for orders\")\n",
    "#def orders():\n",
    "#  return spark.readStream.table(\"orders\")*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b67cfbd6-977f-4ee6-ab27-7236324a83b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT materialized Table"
    }
   },
   "outputs": [],
   "source": [
    "# create a materialized views for customers\n",
    "# create a streaming table for orders\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"bronze\"},\n",
    "  comment = \"Materliazed view for customer bronze table\",\n",
    "  name = \"customer_bronze\"\n",
    ")\n",
    "def cust_bronze():\n",
    "  df = spark.read.table(\"dev.bronze.customer_raw\")\n",
    "  return df\n",
    "\n",
    "\n",
    "#@dlt.create_view(comment = \"Materialized view for customers\")\n",
    "#def customers():\n",
    "#  return spark.readStream.table(\"customers\")\n",
    "# create a view for products\n",
    "#@dlt.create_view(comment = \"View for products\")\n",
    "#def products():\n",
    "#  return spark.readStream.table(\"products\")\n",
    "# create a view for sales\n",
    "#@dlt.create_view(comment = \"View for sales\")\n",
    "#def sales():\n",
    "#  return spark.readStream.table(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2082966a-b09f-4ac9-8c73-e1dce1fcfb7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT joined view for Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# create a viewto join orders with customers\n",
    "@dlt.view(\n",
    "    comment = \"Materliazed view for customer bronze table\"\n",
    ")\n",
    "def joined_vw():\n",
    "  df_c = spark.read.table(\"LIVE.customer_bronze\")\n",
    "  df_o = spark.read.table(\"LIVE.orders_bronze\")\n",
    "\n",
    "  df_join = df_o.join(df_c, how = \"left_outer\", on = df_o.o_custkey == df_c.c_custkey)\n",
    "  return df_join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef40f25e-4b96-4595-98ef-e735ef2fe79b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add a column in silver"
    }
   },
   "outputs": [],
   "source": [
    "# create MV to add new column\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"silver\"},\n",
    "  comment = \"joined table\",\n",
    "  name = \"joined_silver\"\n",
    ")\n",
    "def joined_silver():\n",
    "  df = spark.read.table(\"LIVE.joined_vw\").withColumn(\"__insert_date\", current_timestamp())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ca05e9f-e4ff-4e55-b87d-870156349b2c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Gold Aggregate"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate based on c_mktsegment and find the count of order (c_orderkey)\n",
    "from pyspark.sql.functions import current_timestamp, count\n",
    "\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"gold\"},\n",
    "  comment = \"orders aggregated able\"\n",
    ")\n",
    "def orders_agg_gold():\n",
    "  df = spark.read.table(\"LIVE.joined_silver\")\n",
    "\n",
    "  df_final = df.groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"sum_orders\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "  return df_final"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_dlt_introduction_first_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}