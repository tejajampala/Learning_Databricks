{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dcab28e-f1d5-470b-be61-2b0d53aa3b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Using AutoLoader to read the files data for orders\n",
    "2. To union two streaming tables, we need to use Append Flows (Delta table and Auto Loader table)\n",
    "3. We can use union, but it will read all the files and incremental logic will not work.\n",
    "4. Adding a configuration, and using that to create gold tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3890f38-5b62-430a-bb6d-4ee097587741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Work Types"
    }
   },
   "outputs": [],
   "source": [
    "# DLT works with 3 types of datasets\n",
    "# Streaming Tables (Permanent/Temporary) - used as Append Data Sources, Incremental data\n",
    "# Materialized views - used for transformation, aggregations or computations\n",
    "# Views - used for intermediate transformations, not stored in target schema\n",
    "\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2bc773-6ba0-4970-bbd5-d1309cb37e82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Configuration"
    }
   },
   "outputs": [],
   "source": [
    "_order_status = spark.conf.get(\"custom.orderStatus\", \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ea5d5a-e17f-48a5-af98-c4d849abd364",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Streaming table"
    }
   },
   "outputs": [],
   "source": [
    "# create a streaming table for orders\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"bronze\"},\n",
    "  comment = \"Streaming table for orders bronze table\"\n",
    ")\n",
    "def orders_bronze():\n",
    "  df = spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "  return df\n",
    "\n",
    "#@dlt.create_streaming_table(comment = \"Streaming table for orders\")\n",
    "#def orders():\n",
    "#  return spark.readStream.table(\"orders\")*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ca6d34a-2051-416d-9e57-93393dfd6e51",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT orders Auto Loader"
    }
   },
   "outputs": [],
   "source": [
    "# create a streaming table for orders AutoLoader\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"bronze\"},\n",
    "  comment = \"Streaming table for orders AutoLoader\",\n",
    "  name = \"orders_autoloader_bronze\"\n",
    ")\n",
    "def func():\n",
    "  df = (\n",
    "      spark\n",
    "      .readStream\n",
    "      .format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.SchemaHints\", \"o_orderkey long, o_custkey long, o_orderstatus string, o_totalprice decimal(18,2), o_orderdate date, o_orderpriority string, o_clerk string, o_shippriority integer, o_comment string\")\n",
    "      .option(\"cloudFiles.schemaLocation\", \"/Volumes/dev/etl/landing/autoloader/schemas/1/\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"pathGlobFilter\", \"*.csv\")\n",
    "      .option(\"cloudFilesSchemaEvolutionMode\", \"none\")\n",
    "      .load(\"/Volumes/dev/etl/landing/files/\") \n",
    "    )  \n",
    "  return df\n",
    "\n",
    "#@dlt.create_streaming_table(comment = \"Streaming table for orders\")\n",
    "#def orders():\n",
    "#  return spark.readStream.table(\"orders\")*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56e9511-4e9d-4fc9-a2e2-369607e9441f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Blank streaming table"
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\"orders_union_bronze\")\n",
    "\n",
    "#Append flow - First Table\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    ")\n",
    "def order_delta_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_bronze\")\n",
    "    return df\n",
    "\n",
    "#Append flow  - Second Table\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    ")\n",
    "def order_autoloader_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_autoloader_bronze\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b67cfbd6-977f-4ee6-ab27-7236324a83b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT materialized Table"
    }
   },
   "outputs": [],
   "source": [
    "# create a materialized views for customers\n",
    "# create a streaming table for orders\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"bronze\"},\n",
    "  comment = \"Materliazed view for customer bronze table\",\n",
    "  name = \"customer_bronze\"\n",
    ")\n",
    "def cust_bronze():\n",
    "  df = spark.read.table(\"dev.bronze.customer_raw\")\n",
    "  return df\n",
    "\n",
    "\n",
    "#@dlt.create_view(comment = \"Materialized view for customers\")\n",
    "#def customers():\n",
    "#  return spark.readStream.table(\"customers\")\n",
    "# create a view for products\n",
    "#@dlt.create_view(comment = \"View for products\")\n",
    "#def products():\n",
    "#  return spark.readStream.table(\"products\")\n",
    "# create a view for sales\n",
    "#@dlt.create_view(comment = \"View for sales\")\n",
    "#def sales():\n",
    "#  return spark.readStream.table(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2082966a-b09f-4ac9-8c73-e1dce1fcfb7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT joined view for Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# create a viewto join orders with customers\n",
    "@dlt.view(\n",
    "    comment = \"Bronze Joined table\"\n",
    ")\n",
    "def joined_vw():\n",
    "  df_c = spark.read.table(\"LIVE.customer_bronze\")\n",
    "  df_o = spark.read.table(\"LIVE.orders_union_bronze\")\n",
    "\n",
    "  df_join = df_o.join(df_c, how = \"left_outer\", on = df_o.o_custkey == df_c.c_custkey)\n",
    "  return df_join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef40f25e-4b96-4595-98ef-e735ef2fe79b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add a column in silver"
    }
   },
   "outputs": [],
   "source": [
    "# create MV to add new column\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"silver\"},\n",
    "  comment = \"joined table\",\n",
    "  name = \"orders_silver\"\n",
    ")\n",
    "def orders_silver():\n",
    "  df = spark.read.table(\"LIVE.joined_vw\").withColumn(\"__insert_date\", current_timestamp())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ca05e9f-e4ff-4e55-b87d-870156349b2c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Gold Aggregate"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate based on c_mktsegment and find the count of order (c_orderkey)\n",
    "from pyspark.sql.functions import current_timestamp, count, sum\n",
    "\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"gold\"},\n",
    "  comment = \"orders aggregated able\"\n",
    ")\n",
    "def orders_agg_gold():\n",
    "  df = spark.read.table(\"LIVE.orders_silver\")\n",
    "\n",
    "  df_final = df.groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"count_orders\"),sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "  return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "985bef54-27f3-437b-98ce-e49efa6854fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Gold Tables based on Status"
    }
   },
   "outputs": [],
   "source": [
    "https://adb-7405616295561001.1.azuredatabricks.net/editor/notebooks/4055779692625112?contextId=pipeline%3A34865939-5be6-4478-bd91-0c0288a607b8&o=7405616295561001$0for _status in _order_status.split(\",\"):\n",
    "    @dlt.table(\n",
    "    table_properties = {\"quality\" : \"gold\"},\n",
    "    comment = \"orders aggregated able\",\n",
    "    name = f\"orders_agg_{_status}_gold\"\n",
    "    )\n",
    "    def orders_agg_gold():\n",
    "       df = spark.read.table(\"LIVE.orders_silver\")\n",
    "\n",
    "       df_final = df.where(f\"o_orderstatus = '{_status}'\").groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"count_orders\"),sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "       return df_final\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_dlt_introduction_append_flow_and_config_parameters",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}