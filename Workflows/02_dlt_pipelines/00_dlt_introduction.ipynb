{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dcab28e-f1d5-470b-be61-2b0d53aa3b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.Data Quality and Expectations - only in Table and views\n",
    "@dlt.expect_all(__order_rules) #warn\n",
    "@dlt.expect_all_or_fail(__order_rules) #fail\n",
    "@dlt.expect_all_or_drop(__order_rules) #drop\n",
    "\n",
    "2.Multiple different Expectaions can defined at a same place (view or table)\n",
    "\n",
    "3.If a table is being truncated and new records are inserted, use \"skipChangeCommits\" to \"true\" to make the streaming read work\n",
    "\n",
    "4.If we do not want a full refrsh for a table - use \"pipeline.reset.allowed\" : \"false\" in table properties\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3890f38-5b62-430a-bb6d-4ee097587741",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Work Types"
    }
   },
   "outputs": [],
   "source": [
    "# DLT works with 3 types of datasets\n",
    "# Streaming Tables (Permanent/Temporary) - used as Append Data Sources, Incremental data\n",
    "# Materialized views - used for transformation, aggregations or computations\n",
    "# Views - used for intermediate transformations, not stored in target schema\n",
    "\n",
    "import dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a2bc773-6ba0-4970-bbd5-d1309cb37e82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Configuration"
    }
   },
   "outputs": [],
   "source": [
    "_order_status = spark.conf.get(\"custom.orderStatus\", \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f5065d3-055a-4b40-8298-92204fb000ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rules for Data Quality (warn, drop and fail)\n",
    "__order_rules = {\n",
    "    \"Valid Order Status\" : \"o_orderstatus in ('F', 'O', 'P')\",\n",
    "    \"Valid Order Price\" : \"o_totalprice > 0\"\n",
    "}\n",
    "\n",
    "__customer_rules = {\n",
    "    \"Valid Market Segment\" : \"c_mktsegment is not null\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5ea5d5a-e17f-48a5-af98-c4d849abd364",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Streaming table"
    }
   },
   "outputs": [],
   "source": [
    "# create a streaming table for orders\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"bronze\"},\n",
    "  comment = \"Streaming table for orders bronze table\"\n",
    ")\n",
    "#@dlt.expect_all(__order_rules) #warn\n",
    "#@dlt.expect_all_or_fail(__order_rules) #fail\n",
    "@dlt.expect_all_or_drop(__order_rules) #drop\n",
    "def orders_bronze():\n",
    "  df = spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "  return df\n",
    "\n",
    "#@dlt.create_streaming_table(comment = \"Streaming table for orders\")\n",
    "#def orders():\n",
    "#  return spark.readStream.table(\"orders\")*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ca6d34a-2051-416d-9e57-93393dfd6e51",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT orders Auto Loader"
    }
   },
   "outputs": [],
   "source": [
    "# create a streaming table for orders AutoLoader\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"bronze\"},\n",
    "  comment = \"Streaming table for orders AutoLoader\",\n",
    "  name = \"orders_autoloader_bronze\"\n",
    ")\n",
    "def func():\n",
    "  df = (\n",
    "      spark\n",
    "      .readStream\n",
    "      .format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.SchemaHints\", \"o_orderkey long, o_custkey long, o_orderstatus string, o_totalprice decimal(18,2), o_orderdate date, o_orderpriority string, o_clerk string, o_shippriority integer, o_comment string\")\n",
    "      .option(\"cloudFiles.schemaLocation\", \"/Volumes/dev/etl/landing/autoloader/schemas/1/\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"pathGlobFilter\", \"*.csv\")\n",
    "      .option(\"cloudFilesSchemaEvolutionMode\", \"none\")\n",
    "      .load(\"/Volumes/dev/etl/landing/files/\") \n",
    "    )  \n",
    "  return df\n",
    "\n",
    "#@dlt.create_streaming_table(comment = \"Streaming table for orders\")\n",
    "#def orders():\n",
    "#  return spark.readStream.table(\"orders\")*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56e9511-4e9d-4fc9-a2e2-369607e9441f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Blank streaming table"
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\"orders_union_bronze\")\n",
    "\n",
    "#Append flow - First Table\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    ")\n",
    "def order_delta_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_bronze\")\n",
    "    return df\n",
    "\n",
    "#Append flow  - Second Table\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    ")\n",
    "def order_autoloader_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_autoloader_bronze\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b67cfbd6-977f-4ee6-ab27-7236324a83b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT materialized Table"
    }
   },
   "outputs": [],
   "source": [
    "# create a materialized views for customers\n",
    "# create a streaming table for orders\n",
    "#@dlt.table(\n",
    "#  table_properties = {\"quality\" : \"bronze\"},\n",
    "#  comment = \"Materliazed view for customer bronze table\",\n",
    "#  name = \"customer_bronze\"\n",
    "#)\n",
    "#def cust_bronze():\n",
    "#  df = spark.read.table(\"dev.bronze.customer_raw\")\n",
    "#  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e77f735d-2efb-4d57-9122-b0ac62b22a87",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Customer Table to View"
    }
   },
   "outputs": [],
   "source": [
    "# create a materialized views for customers\n",
    "# create a streaming table for orders\n",
    "@dlt.view(\n",
    "  comment = \"customer bronze view\"\n",
    ")\n",
    "@dlt.expect_all(__customer_rules)\n",
    "def customer_bronze_vw():\n",
    "  df = spark.readStream.table(\"dev.bronze.customer_raw\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a3feba-67b0-419b-bb03-7f416baf172d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SCD1 streaming table"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "dlt.create_streaming_table(\"customer_scd1_bronze\")\n",
    "\n",
    "# SCD 1 Customer\n",
    "dlt.apply_changes(\n",
    "    target = \"customer_scd1_bronze\",\n",
    "    source = \"customer_bronze_vw\",\n",
    "    keys = [\"c_custkey\"],\n",
    "    stored_as_scd_type = 1,\n",
    "    apply_as_deletes = expr(\"__src_action ='D'\"),\n",
    "    apply_as_truncates = expr(\"__src_action ='T'\"),\n",
    "    sequence_by = \"__src_insert_dt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a1f231f-fb5e-4829-80c0-cc4b48ec3a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\"customer_scd2_bronze\")\n",
    "\n",
    "# SCD 2 Customer\n",
    "dlt.apply_changes(\n",
    "    target = \"customer_scd2_bronze\",\n",
    "    source = \"customer_bronze_vw\",\n",
    "    keys = [\"c_custkey\"],\n",
    "    stored_as_scd_type = 2,\n",
    "    except_column_list = [\"__src_action\", \"__src_insert_dt\"],\n",
    "    sequence_by = \"__src_insert_dt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2082966a-b09f-4ac9-8c73-e1dce1fcfb7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT joined view for Bronze"
    }
   },
   "outputs": [],
   "source": [
    "# create a viewto join orders with customers\n",
    "@dlt.view(\n",
    "    comment = \"Bronze Joined table\"\n",
    ")\n",
    "def joined_vw():\n",
    "  df_c = spark.read.table(\"LIVE.customer_scd2_bronze\").where(\"__END_AT IS NULL\")\n",
    "  df_o = spark.read.table(\"LIVE.orders_union_bronze\")\n",
    "\n",
    "  df_join = df_o.join(df_c, how = \"left_outer\", on = df_o.o_custkey == df_c.c_custkey)\n",
    "  return df_join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef40f25e-4b96-4595-98ef-e735ef2fe79b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add a column in silver"
    }
   },
   "outputs": [],
   "source": [
    "# create MV to add new column\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"silver\"},\n",
    "  comment = \"joined table\",\n",
    "  name = \"orders_silver\"\n",
    ")\n",
    "def orders_silver():\n",
    "  df = spark.read.table(\"LIVE.joined_vw\").withColumn(\"__insert_date\", current_timestamp())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ca05e9f-e4ff-4e55-b87d-870156349b2c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DLT Gold Aggregate"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate based on c_mktsegment and find the count of order (c_orderkey)\n",
    "from pyspark.sql.functions import current_timestamp, count, sum\n",
    "\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\" : \"gold\"},\n",
    "  comment = \"orders aggregated able\"\n",
    ")\n",
    "def orders_agg_gold():\n",
    "  df = spark.read.table(\"LIVE.orders_silver\")\n",
    "\n",
    "  df_final = df.groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"count_orders\"),sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "  return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "985bef54-27f3-437b-98ce-e49efa6854fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Gold Tables based on Status"
    }
   },
   "outputs": [],
   "source": [
    "for _status in _order_status.split(\",\"):\n",
    "    @dlt.table(\n",
    "    table_properties = {\"quality\" : \"gold\"},\n",
    "    comment = \"orders aggregated able\",\n",
    "    name = f\"orders_agg_{_status}_gold\"\n",
    "    )\n",
    "    def orders_agg_gold():\n",
    "       df = spark.read.table(\"LIVE.orders_silver\")\n",
    "\n",
    "       df_final = df.where(f\"o_orderstatus = '{_status}'\").groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"count_orders\"),sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "       return df_final\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_dlt_introduction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}